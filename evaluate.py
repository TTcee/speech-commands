# evaluate.py
"""
–ú–û–î–£–õ–¨: evaluate.py
===================
–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è:
------------
- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å
- –û—Ü—ñ–Ω—é—î —ó—ó —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
- –í–∏–º—ñ—Ä—é—î —Å–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É (latency)
"""

import time                              # –î–ª—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —á–∞—Å—É —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É
import torch                             # PyTorch –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –º–æ–¥–µ–ª–ª—é —ñ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏
import numpy as np                       # NumPy –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –º–∞—Å–∏–≤–∞–º–∏ —Ç–∞ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ
from data_loader import load_data        # –Ü–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è test_loader
from model import SpeechCommandCNN       # –Ü–º–ø–æ—Ä—Ç –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ


# === 1. –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ ===
def calculate_accuracy(model, test_loader, device):
    model.eval()                         # –ü–µ—Ä–µ–≤–æ–¥–∏–º–æ –º–æ–¥–µ–ª—å —É —Ä–µ–∂–∏–º –æ—Ü—ñ–Ω–∫–∏
    correct, total = 0, 0                # –õ—ñ—á–∏–ª—å–Ω–∏–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∏—Ö —Ç–∞ –≤—Å—ñ—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π

    with torch.no_grad():                # –í–∏–º–∏–∫–∞—î–º–æ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
        for inputs, labels in test_loader:             # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –≤—Å—ñ –±–∞—Ç—á—ñ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            inputs = inputs.to(device)                 # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ —Ç–µ–Ω–∑–æ—Ä–∏ –Ω–∞ CPU/GPU
            labels = torch.tensor([["yes", "no", "up", "down"].index(l)
                                    for l in labels]).to(device)  # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –º—ñ—Ç–æ–∫ —É —ñ–Ω–¥–µ–∫—Å–∏

            outputs = model(inputs)                    # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ–≥–Ω–æ–∑ –º–æ–¥–µ–ª—ñ
            _, predicted = torch.max(outputs.data, 1)  # –û–±–∏—Ä–∞—î–º–æ –∫–ª–∞—Å –∑ –Ω–∞–π–±—ñ–ª—å—à–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º

            total += labels.size(0)                    # –î–æ–¥–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –±–∞—Ç—á—ñ
            correct += (predicted == labels).sum().item()  # –†–∞—Ö—É—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

    accuracy = 100 * correct / total                   # –û–±—á–∏—Å–ª—é—î–º–æ —Ç–æ—á–Ω—ñ—Å—Ç—å —É %
    return accuracy


# === 2. –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –∑–∞—Ç—Ä–∏–º–∫–∏ (latency) ===
def measure_latency(model, test_loader, device, num_batches=10):
    model.eval()                                       # –†–µ–∂–∏–º –æ—Ü—ñ–Ω–∫–∏
    latencies = []                                     # –ú–∞—Å–∏–≤ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —á–∞—Å—É –∫–æ–∂–Ω–æ–≥–æ –±–∞—Ç—á—É

    with torch.no_grad():
        for i, (inputs, _) in enumerate(test_loader):  # –ü–µ—Ä–µ–±—ñ—Ä –±–∞—Ç—á—ñ–≤
            if i >= num_batches:                       # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –±–∞—Ç—á—ñ–≤
                break

            inputs = inputs.to(device)
            start = time.time()                        # –ü–æ—á–∞—Ç–æ–∫ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è
            _ = model(inputs)                          # –ü—Ä–æ–≥—ñ–Ω –º–æ–¥–µ–ª—ñ
            end = time.time()                          # –ö—ñ–Ω–µ—Ü—å –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è

            latency = (end - start) / len(inputs) * 1000  # –ó–∞—Ç—Ä–∏–º–∫–∞ –¥–ª—è 1 –ø—Ä–∏–∫–ª–∞–¥—É –≤ –º—ñ–ª—ñ—Å–µ–∫—É–Ω–¥–∞—Ö
            latencies.append(latency)                  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç

    avg_latency = np.mean(latencies)                   # –°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞—Ç—Ä–∏–º–∫–∏
    return avg_latency


# === 3. –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –æ—Ü—ñ–Ω–∫–∏ ===
def evaluate_model():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # –í–∏–±–∏—Ä–∞—î–º–æ CPU –∞–±–æ GPU
    print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    _, test_loader = load_data(batch_size=32)          # –ë–µ—Ä–µ–º–æ –ª–∏—à–µ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    model = SpeechCommandCNN(num_classes=4).to(device) # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
    model.load_state_dict(torch.load("saved_model/model.pth", map_location=device))  # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤–∞–≥–∏
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑ saved_model/model.pth")

    # –û—Ü—ñ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç—ñ
    accuracy = calculate_accuracy(model, test_loader, device)
    print(f"üéØ –¢–æ—á–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ: {accuracy:.2f}%")

    # –í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è latency
    avg_latency = measure_latency(model, test_loader, device)
    print(f"‚ö° –°–µ—Ä–µ–¥–Ω—è –∑–∞—Ç—Ä–∏–º–∫–∞ (latency): {avg_latency:.2f} –º—Å / –ø—Ä–∏–∫–ª–∞–¥")


# === 4. –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É ===
if __name__ == "__main__":
    evaluate_model()                      # –ó–∞–ø—É—Å–∫ –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ
