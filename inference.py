# inference.py
"""
–ú–û–î–£–õ–¨: inference.py
====================
–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è:
------------
- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å
- –ü—Ä–∏–π–º–∞—î –∞—É–¥—ñ–æ –∑ —Ñ–∞–π–ª—É –∞–±–æ –º—ñ–∫—Ä–æ—Ñ–æ–Ω–∞
- –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î –π–æ–≥–æ —É –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º—É
- –í–∏–≤–æ–¥–∏—Ç—å —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω—É –∫–æ–º–∞–Ω–¥—É ("yes", "no", "up", "down")
"""

import torch                               # PyTorch –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –º–æ–¥–µ–ª–ª—é —Ç–∞ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏
import torchaudio                          # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∞—É–¥—ñ–æ—Ñ–∞–π–ª—ñ–≤
import sounddevice as sd                   # –ó–∞–ø–∏—Å –∞—É–¥—ñ–æ –∑ –º—ñ–∫—Ä–æ—Ñ–æ–Ω–∞
import numpy as np                         # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —É —Ä–∞–∑—ñ –æ–±—Ä–æ–±–∫–∏ –º–∞—Å–∏–≤—ñ–≤ (–º–æ–∂–Ω–∞ –ø—Ä–∏–±—Ä–∞—Ç–∏, —è–∫—â–æ –Ω–µ —Ç—Ä–µ–±–∞)
from model import SpeechCommandCNN         # –Ü–º–ø–æ—Ä—Ç –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ
from torchaudio.transforms import MelSpectrogram, AmplitudeToDB  # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞—É–¥—ñ–æ —É –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–∏


# === 1. –ö–ª–∞—Å–∏ –∫–æ–º–∞–Ω–¥ ===
LABELS = ["yes", "no", "up", "down"]       # –°–ø–∏—Å–æ–∫ –º–æ–∂–ª–∏–≤–∏—Ö –∫–æ–º–∞–Ω–¥, —â–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É—î –º–æ–¥–µ–ª—å


# === 2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è –∞—É–¥—ñ–æ —É –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º—É ===
def preprocess_audio(waveform, sample_rate=16000):
    transform = torch.nn.Sequential(       # –ü–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω—å
        MelSpectrogram(sample_rate=sample_rate, n_mels=64),  # –ú–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–∞
        AmplitudeToDB()                   # –ü–µ—Ä–µ—Ö—ñ–¥ –¥–æ –¥–µ—Ü–∏–±–µ–ª—ñ–≤
    )
    spec = transform(waveform)            # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–æ —Å–∏–≥–Ω–∞–ª—É
    return spec                           # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º—É


# === 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ ===
def load_model(device):
    model = SpeechCommandCNN(num_classes=len(LABELS)).to(device)  # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å —ñ –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π
    model.load_state_dict(torch.load("saved_model/model.pth", map_location=device))  # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤–∞–≥–∏
    model.eval()                            # –ü–µ—Ä–µ–º–∏–∫–∞—î–º–æ –º–æ–¥–µ–ª—å —É —Ä–µ–∂–∏–º —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É
    return model                            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –≥–æ—Ç–æ–≤—É –º–æ–¥–µ–ª—å


# === 4. –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å —ñ–∑ WAV-—Ñ–∞–π–ª—É ===
def predict_from_file(filepath, model, device):
    waveform, sample_rate = torchaudio.load(filepath)   # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ –∑ —Ñ–∞–π–ª—É
    if waveform.shape[0] > 1:                           # –Ø–∫—â–æ –∫—ñ–ª—å–∫–∞ –∫–∞–Ω–∞–ª—ñ–≤ (—Å—Ç–µ—Ä–µ–æ)
        waveform = waveform.mean(dim=0, keepdim=True)   # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —É –º–æ–Ω–æ

    spec = preprocess_audio(waveform, sample_rate)      # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∞—É–¥—ñ–æ –≤ –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º—É
    spec = spec.unsqueeze(0).to(device)                 # –î–æ–¥–∞—î–º–æ batch-–≤–∏–º—ñ—Ä: [1, 1, 64, time]

    with torch.no_grad():                               # –í–∏–º–∏–∫–∞—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É
        outputs = model(spec)                           # –û—Ç—Ä–∏–º—É—î–º–æ –ª–æ–≥—ñ—Ç–∏ –≤—ñ–¥ –º–æ–¥–µ–ª—ñ
        _, predicted = torch.max(outputs, 1)            # –û–±–∏—Ä–∞—î–º–æ –∫–ª–∞—Å –∑ –Ω–∞–π–±—ñ–ª—å—à–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º
        label = LABELS[predicted.item()]                # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—É –º—ñ—Ç–∫—É –∑–∞ —ñ–Ω–¥–µ–∫—Å–æ–º
    return label                                        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω—É –∫–æ–º–∞–Ω–¥—É


# === 5. –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å —ñ–∑ –º—ñ–∫—Ä–æ—Ñ–æ–Ω–∞ ===
def predict_from_mic(model, device, duration=1.0, sample_rate=16000):
    print("üé§ –ó–∞–ø–∏—Å –∑–≤—É–∫—É... –ì–æ–≤–æ—Ä–∏ –∫–æ–º–∞–Ω–¥—É (yes/no/up/down)...")
    recording = sd.rec(                                 # –ó–∞–ø–∏—Å—É—î–º–æ –∑–≤—É–∫ –∑ –º—ñ–∫—Ä–æ—Ñ–æ–Ω–∞
        int(duration * sample_rate),                    # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–µ–º–ø–ª—ñ–≤ = —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å * —á–∞—Å—Ç–æ—Ç–∞
        samplerate=sample_rate,
        channels=1,
        dtype='float32'
    )
    sd.wait()                                           # –ß–µ–∫–∞—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –∑–∞–ø–∏—Å—É
    waveform = torch.tensor(recording.T)                # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–ø–∏—Å —É —Ç–µ–Ω–∑–æ—Ä —ñ —Ç—Ä–∞–Ω—Å–ø–æ–Ω—É—î–º–æ
    spec = preprocess_audio(waveform, sample_rate)      # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —É –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º—É
    spec = spec.unsqueeze(0).to(device)                 # –î–æ–¥–∞—î–º–æ batch-–≤–∏–º—ñ—Ä

    with torch.no_grad():
        outputs = model(spec)                           # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
        _, predicted = torch.max(outputs, 1)            # –û–±–∏—Ä–∞—î–º–æ –Ω–∞–π–±—ñ–ª—å—à –π–º–æ–≤—ñ—Ä–Ω–∏–π –∫–ª–∞—Å
        label = LABELS[predicted.item()]                # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—É –º—ñ—Ç–∫—É

    print(f"üîä –†–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ: {label}")                    # –í–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É –∫–æ–Ω—Å–æ–ª—å
    return label                                        # –Ü –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —ó—ó –∑ —Ñ—É–Ω–∫—Ü—ñ—ó


# === 6. –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É ===
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # –í–∏–±–∏—Ä–∞—î–º–æ CPU –∞–±–æ GPU
    model = load_model(device)                                             # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞.")

    # –¶–∏–∫–ª, —â–æ–± –º–æ–∂–Ω–∞ –±—É–ª–æ –±–∞–≥–∞—Ç–æ—Ä–∞–∑–æ–≤–æ —Ç–µ—Å—Ç—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å
    while True:
        print("\n–í–∏–±–µ—Ä–∏ —Ä–µ–∂–∏–º:")
        print("  1 ‚Äî —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ –∫–æ–º–∞–Ω–¥—É –∑ WAV-—Ñ–∞–π–ª—É")
        print("  2 ‚Äî —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ –∫–æ–º–∞–Ω–¥—É –∑ –º—ñ–∫—Ä–æ—Ñ–æ–Ω–∞")
        print("  q ‚Äî –≤–∏–π—Ç–∏ –∑ –ø—Ä–æ–≥—Ä–∞–º–∏")
        choice = input("–¢–≤—ñ–π –≤–∏–±—ñ—Ä: ")

        if choice == "1":
            path = input("–í–≤–µ–¥–∏ —à–ª—è—Ö –¥–æ WAV-—Ñ–∞–π–ª—É: ")   # –ó–∞–ø–∏—Ç—É—î–º–æ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
            result = predict_from_file(path, model, device)  # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ–≥–Ω–æ–∑
            print(f"üîä –†–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ –∫–æ–º–∞–Ω–¥—É: {result}")   # –í–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç

        elif choice == "2":
            predict_from_mic(model, device)             # –ó–∞–ø–∏—Å —ñ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –∑ –º—ñ–∫—Ä–æ—Ñ–æ–Ω–∞

        elif choice.lower() == "q":                     # –Ø–∫—â–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á —Ö–æ—á–µ –≤–∏–π—Ç–∏
            print("üö™ –í–∏—Ö—ñ–¥ –∑ –ø—Ä–æ–≥—Ä–∞–º–∏.")
            break                                       # –í–∏—Ö–æ–¥–∏–º–æ –∑ —Ü–∏–∫–ª—É

        else:
            print("‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –°–ø—Ä–æ–±—É–π —â–µ —Ä–∞–∑.") # –ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
