# train.py
"""
–ú–û–î–£–õ–¨: train.py
=================
–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è:
------------
- –ó–∞–ø—É—Å–∫–∞—î –ø—Ä–æ—Ü–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –∞—É–¥—ñ–æ-–∫–æ–º–∞–Ω–¥–∞—Ö
- –í–∏–≤–æ–¥–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Ç—Ä–∞—Ç (loss) —ñ —Ç–æ—á–Ω—ñ—Å—Ç—å
- –ó–±–µ—Ä—ñ–≥–∞—î –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å —É —Ñ–∞–π–ª model.pth
"""

import os                           # –†–æ–±–æ—Ç–∞ –∑ —Ñ–∞–π–ª–æ–≤–æ—é —Å–∏—Å—Ç–µ–º–æ—é (—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞–ø–∫–∏ –¥–ª—è –º–æ–¥–µ–ª—ñ)
import torch                        # –û—Å–Ω–æ–≤–Ω–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ PyTorch
import torch.nn as nn               # –ù–µ–π—Ä–æ–º–µ—Ä–µ–∂–µ–≤—ñ —à–∞—Ä–∏ —Ç–∞ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç
import torch.optim as optim         # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∏ (Adam)
from tqdm import tqdm               # –ü—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä –¥–ª—è –Ω–∞–æ—á–Ω–æ–≥–æ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

from data_loader import load_data   # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö —ñ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
from model import SpeechCommandCNN  # –Ü–º–ø–æ—Ä—Ç –∫–ª–∞—Å—É –º–æ–¥–µ–ª—ñ


# === 1. –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ===
def train_model(epochs=5, batch_size=32, learning_rate=0.001):
    # –ü—Ä–∏—Å—Ç—Ä—ñ–π (GPU —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # –í–∏–±—ñ—Ä CPU –∞–±–æ GPU
    print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")                           # –í–∏–≤—ñ–¥, —è–∫–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    train_loader, test_loader = load_data(batch_size=batch_size)            # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
    model = SpeechCommandCNN(num_classes=4).to(device)                      # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —ñ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–Ω—è –Ω–∞ GPU/CPU

    # –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç —ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
    criterion = nn.CrossEntropyLoss()                                       # –í–∏–±—ñ—Ä —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)            # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä Adam

    # === –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ===
    for epoch in range(epochs):                                             # –¶–∏–∫–ª –ø–æ –µ–ø–æ—Ö–∞—Ö
        model.train()                                                       # –ü–µ—Ä–µ–≤–æ–¥–∏–º–æ –º–æ–¥–µ–ª—å —É —Ä–µ–∂–∏–º —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        running_loss = 0.0                                                  # –ù–∞–∫–æ–ø–∏—á—É–≤–∞—á –≤—Ç—Ä–∞—Ç –∑–∞ –µ–ø–æ—Ö—É
        correct, total = 0, 0                                               # –ü—Ä–∞–≤–∏–ª—å–Ω—ñ —Ç–∞ –∑–∞–≥–∞–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

        progress_bar = tqdm(train_loader, desc=f"–ï–ø–æ—Ö–∞ {epoch+1}/{epochs}") # –ü—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä

        for inputs, labels in progress_bar:                                 # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –±–∞—Ç—á—ñ
            # –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ —É —Ñ–æ—Ä–º–∞—Ç—ñ [batch, 1, 64, time]
            inputs = inputs.to(device)                                      # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU/CPU
            labels = torch.tensor([["yes", "no", "up", "down"].index(l)     # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—É –º—ñ—Ç–∫—É —É —ñ–Ω–¥–µ–∫—Å
                                    for l in labels]).to(device)

            # –û–±–Ω—É–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
            optimizer.zero_grad()                                           # –°–∫–∏–¥–∞—î–º–æ —Å—Ç–∞—Ä—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏

            # –ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥
            outputs = model(inputs)                                         # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –±–∞—Ç—á —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å

            # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç
            loss = criterion(outputs, labels)                               # –û–±—á–∏—Å–ª—é—î–º–æ loss
            loss.backward()                                                 # –ó–≤–æ—Ä–æ—Ç–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏
            optimizer.step()                                                # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥ –º–æ–¥–µ–ª—ñ

            running_loss += loss.item()                                     # –î–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—ñ –≤—Ç—Ä–∞—Ç–∏

            # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ
            _, predicted = torch.max(outputs.data, 1)                       # –û–±–∏—Ä–∞—î–º–æ –∫–ª–∞—Å –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é
            total += labels.size(0)                                         # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
            correct += (predicted == labels).sum().item()                   # –°–∫—ñ–ª—å–∫–∏ –º–æ–¥–µ–ª—å –≤–≥–∞–¥–∞–ª–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ

            progress_bar.set_postfix(loss=loss.item(),                      # –û–Ω–æ–≤–ª—é—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä
                                     acc=f"{100 * correct / total:.2f}%")

        print(f"–ï–ø–æ—Ö–∞ [{epoch+1}/{epochs}] - –í—Ç—Ä–∞—Ç–∞: {running_loss/len(train_loader):.4f} "
              f"| –¢–æ—á–Ω—ñ—Å—Ç—å: {100*correct/total:.2f}%")                      # –ü—ñ–¥—Å—É–º–æ–∫ –µ–ø–æ—Ö–∏

    # === 2. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ ===
    os.makedirs("saved_model", exist_ok=True)                               # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–∞–ø–∫—É, —è–∫—â–æ —ó—ó –Ω–µ–º–∞—î
    torch.save(model.state_dict(), "saved_model/model.pth")                 # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ
    print("\n‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É saved_model/model.pth")                  # –ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è

    # === 3. –û—Ü—ñ–Ω–∫–∞ –ø—ñ—Å–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ===
    evaluate_model(model, test_loader, device)                              # –ó–∞–ø—É—Å–∫ –æ—Ü—ñ–Ω–∫–∏ –Ω–∞ —Ç–µ—Å—Ç—ñ


# === 4. –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ ===
def evaluate_model(model, test_loader, device):
    model.eval()                                                            # –†–µ–∂–∏–º –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ
    correct, total = 0, 0                                                   # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ç–æ—á–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π

    with torch.no_grad():                                                   # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
        for inputs, labels in test_loader:                                  # –ü—Ä–æ—Ö–æ–¥–∏–º–æ —Ç–µ—Å—Ç–æ–≤—ñ –±–∞—Ç—á—ñ
            inputs = inputs.to(device)
            labels = torch.tensor([["yes", "no", "up", "down"].index(l)
                                    for l in labels]).to(device)

            outputs = model(inputs)                                         # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            _, predicted = torch.max(outputs.data, 1)                       # –í–∏–±—ñ—Ä –∫–ª–∞—Å—É
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    acc = 100 * correct / total                                             # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ
    print(f"\nüéØ –¢–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö: {acc:.2f}%")                    # –í–∏–≤—ñ–¥ —Ç–æ—á–Ω–æ—Å—Ç—ñ
    return acc                                                              # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–æ—á–Ω—ñ—Å—Ç—å


# === 5. –¢–æ—á–∫–∞ –≤—Ö–æ–¥—É ===
if __name__ == "__main__":                                                  # –ó–∞–ø—É—Å–∫ —Ñ–∞–π–ª—É –Ω–∞–ø—Ä—è–º—É
    train_model(epochs=3, batch_size=32, learning_rate=0.001)              # –°—Ç–∞—Ä—Ç –ø—Ä–æ—Ü–µ—Å—É —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
